{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized])\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv('train_test_set2.csv',encoding='latin-1')\n",
    "X = news_df.Article.apply(preprocess_text)\n",
    "y = news_df.NewsType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transforms text to feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text clustering with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2 \n",
      "The means in clusters are : [array([1.23825001e-02, 5.08179922e-04, 8.61327490e-05, ...,\n",
      "       0.00000000e+00, 2.78828636e-03, 8.98243016e-04]), array([0.0038227 , 0.        , 0.        , ..., 0.00247791, 0.        ,\n",
      "       0.        ])]\n",
      "For n_clusters = 3 \n",
      "The means in clusters are : [array([0.00914082, 0.00066931, 0.00011344, ..., 0.        , 0.00367238,\n",
      "       0.        ]), array([0.01043051, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "       0.00110239]), array([0.00209836, 0.        , 0.        , ..., 0.00532292, 0.        ,\n",
      "       0.        ])]\n",
      "For n_clusters = 5 \n",
      "The means in clusters are : [array([0.00450217, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ]), array([0.01643209, 0.00070363, 0.        , ..., 0.        , 0.0038607 ,\n",
      "       0.00124372]), array([0.00396547, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ]), array([0.00335982, 0.        , 0.00031008, ..., 0.        , 0.        ,\n",
      "       0.        ]), array([0.00144754, 0.        , 0.        , ..., 0.00991164, 0.        ,\n",
      "       0.        ])]\n"
     ]
    }
   ],
   "source": [
    "range_n_clusters = [2, 3, 5]\n",
    "for n_clusters in range_n_clusters:\n",
    "    kclusterer = KMeansClusterer(num_means=n_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "    assigned_clusters = kclusterer.cluster(X_tfidf.toarray(),assign_clusters=True)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "         \"\\nThe means in clusters are :\", kclusterer.means(),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
